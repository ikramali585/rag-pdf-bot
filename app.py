import tempfile
import os
import shutil
import streamlit as st
from streamlit_chat import message
from streamlit_extras.colored_header import colored_header

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt = ChatPromptTemplate.from_template(
"""
You are a helpful and knowledgeable assistant, here to answer user questions about the contents of provided PDF documents.
The user may ask anything from a simple keyword to a detailed question.

### **Instructions:**  
- **Answer ONLY based on the provided context.** Avoid making up information or speculating.
- **Provide clear, concise, and direct answers based solely on the information within the document context.**  
- **If the user's query is a general greeting (like "hi", "hello"), just reply with a friendly greeting and do not reference any document content.**
- **If there are links (such as videos or articles) present in the context, include them at the end of your response. Otherwise, do not provide any link.**

### **Context from Document:**  
<context>
{context}
</context>

### **User's Question:**  
{input}
"""
)


st.set_page_config(
    page_title="CharlieQ",
    page_icon="üìò",
    layout="wide",
    initial_sidebar_state="expanded",
)

def create_vector_db(file_uploads):
    """
    Create a vector database from multiple uploaded PDF files.
    
    Args:
        file_uploads (list[st.UploadedFile]): List of Streamlit file upload objects containing the PDFs.
    
    Returns:
        A vector store containing the processed document chunks from all files.
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        # Get API key from environment
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("‚ùå OPENAI_API_KEY is not set. Please configure it in Streamlit secrets.")
            st.stop()
            return None
            
        # Initialize embeddings with API key from environment
        # The API key is already set in os.environ, so we don't need to pass it explicitly
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        all_chunks = []

        # Process each uploaded PDF file
        for file_upload in file_uploads:
            path = os.path.join(temp_dir, file_upload.name)
            
            # Save uploaded file to temporary location
            with open(path, "wb") as f:
                f.write(file_upload.getvalue())

            # Load PDF file using PyMuPDFLoader
            loader = PyMuPDFLoader(path)
            data = loader.load()

            # Split the document into chunks
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            chunks = text_splitter.split_documents(data)
            all_chunks.extend(chunks)

        # Create vector store from all chunks
        vector_db = FAISS.from_documents(all_chunks, embeddings)
        return vector_db
        
    finally:
        # Clean up temporary files
        shutil.rmtree(temp_dir, ignore_errors=True)

def conversational_chat(query):
    """Handle user query and return AI response"""
    result = st.session_state['qa'].invoke({"input": query})
    st.session_state['history'].append((query, result['answer']))
    return result['answer']


def main():
    # Initialize session state
    if 'history' not in st.session_state:
        st.session_state['history'] = []
    
    # Get API keys from Streamlit secrets
    try:
        openai_key = st.secrets["OPENAI_API_KEY"]
        groq_key = st.secrets.get("GROQ_API_KEY", "")
        
        # Set environment variable
        os.environ["OPENAI_API_KEY"] = openai_key
        
        # Verify the key is set
        if not openai_key or openai_key.strip() == "":
            st.error("‚ö†Ô∏è OPENAI_API_KEY is empty. Please configure it in Streamlit secrets.")
            st.stop()
            return
    except KeyError as e:
        st.error(f"‚ö†Ô∏è Missing OPENAI_API_KEY in Streamlit secrets. Error: {e}")
        st.stop()
        return
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error loading secrets: {e}")
        st.stop()
        return

    # UI Setup
    st.title(":violet[Charlie Bot]")
    colored_header(label='', description='', color_name='gray-30')
    
    # Two-column layout
    col1, col2 = st.columns([1.5, 2])
    
    # File uploader
    uploaded_files = col1.file_uploader(
        "Choose PDF files", 
        type="pdf", 
        accept_multiple_files=True
    )

    # Process uploaded files
    if uploaded_files:
        with st.spinner("Processing PDFs and creating vector database..."):
            if 'vectors' not in st.session_state:
                st.session_state['vectors'] = create_vector_db(uploaded_files)
                
                # Initialize LLM with explicit API key
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    st.error("‚ùå OPENAI_API_KEY is not available for LLM initialization")
                    st.stop()
                    return
                    
                # LLM will use the API key from environment
                llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.7
                )
                
                # Setup retriever and chains
                retriever = st.session_state['vectors'].as_retriever(
                    search_type="similarity", 
                    search_kwargs={"k": 5}
                )
                document_chain = create_stuff_documents_chain(llm, prompt)
                st.session_state['qa'] = create_retrieval_chain(retriever, document_chain)
                
                st.success("‚úÖ Vector database created! You can now ask questions.")
                
        st.session_state['ready'] = True

    # Chat interface
    if st.session_state.get('ready', False):
        if 'generated' not in st.session_state:
            st.session_state['generated'] = ["Welcome! You can now ask any questions about your PDFs"]
        if 'past' not in st.session_state:
            st.session_state['past'] = ["Hey!"]

        response_container = col2.container()
        container = col2.container()

        # Query input
        with container:
            with st.form(key='chat_form', clear_on_submit=True):
                user_input = st.text_input(
                    "Query:", 
                    placeholder="Ask me about your documents...", 
                    key='input'
                )
                submit_button = st.form_submit_button(label='Send', use_container_width=True)

            if submit_button and user_input:
                with st.spinner("Thinking..."):
                    output = conversational_chat(user_input)
                st.session_state['past'].append(user_input)
                st.session_state['generated'].append(output)

        # Display chat history
        if st.session_state['generated']:
            with response_container:
                for i in range(len(st.session_state['generated'])):
                    if i < len(st.session_state['past']):
                        # User message
                        st.markdown(
                            f"<div style='background-color: #90caf9; color: black; padding: 10px; border-radius: 5px; width: 70%; float: right; margin: 5px;'>{st.session_state['past'][i]}</div>",
                            unsafe_allow_html=True
                        )
                    # Bot message
                    message(
                        st.session_state["generated"][i], 
                        key=str(i), 
                        avatar_style="fun-emoji"
                    )

if __name__ == "__main__":
    main()
